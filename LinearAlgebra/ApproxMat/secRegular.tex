%!TEX root = ../larxxia.tex


\section{Regularize linear equations}
\label{sec:rle}
\secttoc
\index{linear equation|(}
\index{regularization|(}

\begin{quoted}{\parbox{15em}{Sherlock Holmes, in The Boscombe Valley Mystery, by Sir Arthur Conan Doyle, 1892}}
Singularity is almost invariably a clue.
\end{quoted}


Often we need to approximate the matrix in a linear equation.  
Such approximation is especially likely when the matrix itself comes from experimental measurements and so has errors.
We do not want such errors to affect results.
By avoiding division with small \idx{singular value}s, the procedure developed in this section avoids unwarranted magnification of errors.
Sometimes such error magnification is disastrous, so avoiding it is essential.

\begin{example} \label{eg:2regu}
Suppose from measurements in some experiment we want to solve the two linear equations
\begin{equation*}
0.5x+0.3y=1\quad\text{and}\quad 1.1x+0.7y=2\,,
\end{equation*}
where all the \idx{coefficient}s on \emph{both} the left-hand sides and the right-hand sides are determined from experimental measurements.
Suppose they are measured to \idx{experimental error}s~\(\pm0.05\)\,.
Solve the equations.
\begin{solution} 
Using \cref{pro:unisol} in \script, form the matrix and the right-hand side
\begin{verbatim}
A=[0.5 0.3;1.1 0.7]
b=[1.0;2.0]
\end{verbatim}
Then check the condition number with \verb|rcond(A)| to find it is~\(0.007\) which previously we would call only just outside the `good' range (\cref{pro:unisol}).
\marginpar{\begin{tikzpicture}
  \begin{axis}[footnotesize,font=\footnotesize ,axis equal image
  , xlabel={$x$}, ylabel={$y$}, axis lines=middle,domain=-5:10
  ]
  \addplot[blue] {(1-0.5*x)/0.3};
  \addplot[red] {(2-1.1*x)/0.7};
  \addplot[black,mark=*] coordinates {(5,-5)};
  \end{axis}
\end{tikzpicture}}%
So proceed warily to compute the solution with~\verb|A\b| to find \((x,y)=(5,-5)\) (as illustrated by the intersection of the two lines in the margin).


 Is this solution reasonable?  No.  Not when the matrix itself has errors.
Let's perturb the matrix~\(A\) by amounts consistent with its experimental error of~\(\pm0.05\) and explore the predicted solutions (the first two perturbations illustrated):%
\marginpar{\begin{tikzpicture}
  \begin{axis}[footnotesize,font=\footnotesize ,axis equal image
  , xlabel={$x$}, ylabel={$y$}, axis lines=middle,domain=-5:10
  ]
  \addplot[blue] {(1-0.47*x)/0.29};
  \addplot[red] {(2-1.06*x)/0.68};
  \addplot[black,mark=*] coordinates {(8.2,-9.8)};
  \end{axis}
\end{tikzpicture}}%
\marginpar{\begin{tikzpicture}
  \begin{axis}[footnotesize,font=\footnotesize ,axis equal image
  , xlabel={$x$}, ylabel={$y$}, axis lines=middle,domain=-5:10
  ]
  \addplot[blue] {(1-0.45*x)/0.32};
  \addplot[red] {(2-1.05*x)/0.67};
  \addplot[black,mark=*] coordinates {(-0.9,4.3)};
  \end{axis}
\end{tikzpicture}}%
\begin{eqnarray*}
&&A=\begin{bmatrix} 0.47&0.29\\1.06&0.68 \end{bmatrix}
\implies \xv=\verb|A\b|=(8.2,-9.8);
\\&&A=\begin{bmatrix} 0.45&0.32\\1.05&0.67 \end{bmatrix}
\implies \xv=\verb|A\b|=(-0.9,4.3);
\\&&A=\begin{bmatrix} 0.46&0.31\\1.06&0.73 \end{bmatrix}
\implies \xv=\verb|A\b|=(15.3,-19.4).
\end{eqnarray*}
For equally valid matrices~\(A\), to within experimental error, the predicted solutions are all over the place!


 If the matrix itself has errors, then we \emph{must} reconsider \cref{pro:unisol}.
The \svd\ empowers a sensible resolution.
Compute an \svd\ of the matrix, \(A=\usv\), with \verb|[U,S,V]=svd(A)| to find \twodp
\setbox\ajrqrbox\hbox{\qrcode{% 2x2 approx
A=[0.5 0.3;1.1 0.7]
b=[1.0;2.0]
[U,S,V]=svd(A)
z=U'*b
y=[z(1)/S(1,1);0]
x=V*y
}}%
\marginajrbox%
\begin{equation*}
A=\begin{bmatrix} -0.41 & -0.91
\\-0.91 &  0.41 \end{bmatrix}
\begin{bmatrix} 1.43 & 0
\\0 &  0.01 \end{bmatrix}
\tr{\begin{bmatrix} -0.85 & -0.53
\\-0.53 &  0.85 \end{bmatrix}}
\end{equation*}
Because the matrix~\(A\) has errors~\(\pm0.05\), the small singular value of~\(0.01\) might as well be zero: it is zero to experimental error.
That is, because the matrix~\(A_1\) is a distance \(\|A-A_1\|=0.01\) away from~\(A\), and this distance is less than the experimental error of~\(0.05\), then it is better to solve the equation with the singular~\(A_1\) instead of the original~\(A\).
The appropriate solution algorithm is then \cref{pro:appsol} for inconsistent equations, not \cref{pro:unisol}.
Thus, using the above \svd, \twodp
\begin{enumerate}
\item \(\zv=\tr U\bv=(-2.23,-0.10)\);
\item due to the effectively zero singular value, neglect \(z_2=-0.10\) as an error, and solve
\(\begin{bmat} 1.43 & 0
\\0 &  0 \end{bmat}\yv=\begin{bmat} -2.23\\0 \end{bmat}\)
to deduce \(\yv=(-1.56,y_2)\);
\item consequently, we find reasonable solutions are \(\xv=V\yv=(1.32,0.83)+y_2(-0.53,0.85)\) ---the \idx{parametric equation} of a line (\cref{sec:pel}).
\end{enumerate}
The four different `answers' computed by~\verb|A\b| above are just four different points (nearly) on this line.
Other different `answers' computed by~\verb|A\b| would be other points (nearly) on this line.

 To choose between this infinitude of solutions on the line, extra information must be provided by the context\slash application\slash modelling.  
For example, often one prefers the solution of smallest length\slash magnitude, obtained by setting \(y_2=0\) (\cref{thm:smallsoln}); that is, \(\xv_{\text{smallest}}=(1.32,0.83)\).
\end{solution}
\end{example}




\begin{activity}
The \idx{coefficient}s in the following pair of linear equations are obtained from an experiment and so the coefficients have errors of roughly~\(\pm0.05\):
\begin{equation*}
0.8x+1.1y=4,\quad 0.6x+0.8y=3\,.
\end{equation*}
By checking how well the equations are satisfied, which of the following \emph{cannot} be a plausible solution~\((x,y)\) of the pair of equations?
%\begin{verbatim}
%for i=1:999, A=round(5+rand(2)*10)/10; if abs(cond(A)-200)<100,break, end,end, A=A,condA=cond(A)
%x=[5;0]+round([0.806;-0.592]*randn*10)/5,res=A*x-[4;3]
%\end{verbatim}
\actposs{\((5.6,0.8)\)}{\((3.6,1)\)}{\((5,0)\)}{\((6.6,-1.2)\)}
\end{activity}






\subsection{The SVD illuminates regularization}
\label{sec:svdir}


\begin{quoted}{\index{Feynman, Richard}Richard Feynman}
I think it is much more interesting to live with uncertainty than to live with answers that might be wrong.
\end{quoted}

\begin{procedure}[approximate linear equations] \label{pro:appmat}
Suppose the \idx{system} of \idx{linear equation}s \(A\xv=\bv\) arises from an experiment where both the \(m\times n\) matrix~\(A\) and the right-hand side vector~\bv\ are subject to \idx{experimental error}.  
Suppose the expected error in the matrix and vector \idx{entries} are of \idx{magnitude}~\(\epsilon\).
\begin{aside}
Recall that (\cref{thm:erramp}) the symbol~\idx{$\epsilon$} is the Greek letter epsilon, and often denotes errors.
\end{aside}%
\begin{enumerate}
\item When forming the matrix~\(A\) and vector~\bv, scale the data so that\index{data scaling} 
\begin{itemize}
\item all \(m\times n\)~\idx{components} in~\(A\) have the same physical units, and they are of roughly the same magnitude; and
\item similarly for the \(m\) components of~\bv.
\end{itemize}
Estimate the error~\(\epsilon\) corresponding to this matrix~\(A\).
 
\item Compute an \svd\ \(A=\usv\).

\item Choose `\idx{rank}'~\(k\) to be the number of \idx{singular value}s bigger than the error~\(\epsilon\); that is, \(\sigma_1\geq \sigma_2\geq\cdots \geq \sigma_k>\epsilon>\sigma_{k+1}\geq \cdots\geq 0\)\,.
Then the rank~\(k\) approximation to~\(A\) is
\begin{eqnarray*}
A_k&:=&US_k\tr V
\\&=&\sigma_1\uv_1\tr\vv_1+\sigma_2\uv_2\tr\vv_2+\cdots+\sigma_k\uv_k\tr\vv_k
\\&=&\verb|U(:,1:k)*S(1:k,1:k)*V(:,1:k)'|.
\end{eqnarray*}
But do not construct~\(A_k\) as we only need its \svd\ to solve the system.

\item Solve the approximating \idx{linear equation} \(A_k\xv=\bv\) as 
in \cref{thm:appsol,thm:smallsoln} (often as an 
\index{inconsistent equations}\idx{inconsistent} set of equations).
Usually use the \svd\ \(A_k=US_k\tr V\).

\item Among all the solutions allowed, choose the `best' according to some explicit additional need of the application: often the \idx{smallest solution} overall; or just as often a solution with the most zero \idx{components}.
\end{enumerate}
\end{procedure}


That is, the procedure is to treat as \idx{zero} all \idx{singular value}s smaller than the expected error in the matrix \idx{entries}.
For example, modern computers have nearly sixteen significant decimal digits accuracy, so even in `exact' computation there is a background \idx{relative error} of about~\(10^{-15}\).
Consequently, in computation on modern computers, every singular value smaller than \(10^{-15}\sigma_1\) must be treated as \idx{zero}.
For safety, even in `exact' computation, every \idx{singular value} smaller than say~\(10^{-8}\sigma_1\) should be treated as~\idx{zero}.


\begin{activity}
In some system of linear equations the five \idx{singular value}s of the matrix are
%svd(hilb(5)+0.01*randn(5))
\begin{equation*}
1.5665,\quad
0.2222,\quad
0.0394,\quad
0.0107,\quad
0.0014.
\end{equation*}
Given the matrix components have errors of about~\(0.02\), what is the effective rank of the matrix?
\actposs[4]3124
\end{activity}

The final step in \cref{pro:appmat} arises because in many cases an infinite number of possible solutions are derived.
The linear algebra cannot presume which is best for your application.
Consequently, in future applications you will have to be aware of the freedom, and make a choice based on extra information.
For two examples:
\begin{itemize}
\item in a \textsc{ct}-scan\index{CT scan} such as \cref{eg:ctscan} one would usually prefer the greyest result in order to avoid diagnosing artifices;
\item in the \idx{data mining} task of fitting curves or surfaces to data, one would instead usually prefer a curve or surface with fewest nonzero \idx{coefficient}s.
\end{itemize}
Such extra information from the application is essential. 





\begin{example} \label{eg:3regmata}
For the following matrix~\(A\) and right-hand side vector~\(\bv\),
solve \(A\xv=\bv\)\,.
But suppose the matrix entries come from experiments and are only known to within errors~\(\pm0.05\),  solve \(A'\xv'=\bv\) for some chosen matrices~\(A'\) which approximate~\(A\) to this error.
Finally, use an \svd\ to find a \idx{general solution} consistent with the error in matrix~\(A\).
Report to two decimal places.
\begin{equation*}
A=\begin{bmatrix} -0.2&-0.6&1.8
\\ 0.0&0.2&-0.4
\\ -0.3&0.7&0.3 \end{bmatrix},\quad
\bv=\begin{bmatrix} -0.5
\\ 0.1
\\ -0.2
 \end{bmatrix}.
\end{equation*}
\begin{solution} 
Enter the matrix and vector into \script, note the poor \verb|rcond|, and solve with \verb|x=A\b| to determine \(\xv=(0.06,-0.13,-0.31)\).
\setbox\ajrqrbox\hbox{\qrcode{% 3x3 approx Ax=b
A=[-0.2 -0.6 1.8
   0.0 0.2 -0.4
  -0.3 0.7 0.3 ]
b=[-0.5;0.1;-0.2]
rcond(A)
x=A\slosh b
Ad=A+(rand(3)*9-4.5)/100 
x=Ad\slosh b
[U,S,V]=svd(A)
z=U(:,1:2)'*b
y=z./diag(S(1:2,1:2))
x=V(:,1:2)*y
}}%
\marginajrbox%

To within the experimental error of~\(\pm0.05\) the following two matrices approximate~\(A\): that is, they might have been what was measured for~\(A\).
Then \script\ \verb|x=A\b| gives the corresponding equally valid solutions.
\begin{itemize}
\item \(A'=\begin{bmatrix} -0.16&-0.58&1.83
\\ 0.01&0.16&-0.45
\\ -0.28&0.74&0.30 \end{bmatrix}\) gives
\(\xv'=\begin{bmatrix} 0.85\\0.12\\-0.16 \end{bmatrix}\)
\item \(A''=\begin{bmatrix} -0.22&-0.62&1.77
\\ 0.01&0.17&-0.42
\\ -0.26&0.66&0.26 \end{bmatrix}\) gives
\(\xv''=\begin{bmatrix} 0.42\\-0.04\\-0.24 \end{bmatrix}\)
\end{itemize}
There are major differences between these equally valid solutions~\(\xv\), \(\xv'\) and~\(\xv''\).
The problem is that, relative to the experimental error, there is a small singular value in the matrix~\(A\).
We must use an \svd\ to find all solutions consistent with the experimental error. 
Consequently, compute \verb|[U,S,V]=svd(A)| to find \twodp
\begin{verbatim}
U =
  -0.97   0.03  -0.23
   0.22  -0.10  -0.97
  -0.05  -0.99   0.09
S =
   1.96      0      0
      0   0.82      0
      0      0   0.02
V =
   0.11   0.36   0.93
   0.30  -0.90   0.31
  -0.95  -0.25   0.20
\end{verbatim}
The singular value~\(0.02\) is less than the error~\(\pm0.05\) so is effectively zero.
Hence we solve the system as if this singular value is zero; that is, as if matrix~\(A\) has rank two.
Compute the smallest consistent solution with the three steps \verb|z=U(:,1:2)'*b|, \verb|y=z./diag(S(1:2,1:2))|, and \verb|x=V(:,1:2)*y|.
Then add an arbitrary multiple of the last column of~\verb|V| to determine a general solution 
\begin{equation*}
\xv=(0.10,-0.11,-0.30)+t(0.93,0.31,0.20).
\end{equation*}
\end{solution}
\end{example}


This example gives \idx{infinitely many solutions} which are equally valid as far as the linear algebra is concerned.
In such an example, more information from an application is needed to choose which to \emph{prefer} among the infinity of solutions.



\begin{reduce}
\begin{example}
Repeat \cref{eg:3regmata} with matrix and right-hand side
\begin{equation*}
A=\begin{bmatrix} -1.1&0.1&0.7&-0.1
\\0.1&-0.1&1.2&-0.6
\\0.8&-0.2&0.4&-0.8
\\0.8&0.1&-2.0&1.0 \end{bmatrix},\quad
\bv=\begin{bmatrix} -1.1
\\-0.1
\\1.1
\\0.8
 \end{bmatrix}
\end{equation*}
\begin{solution} 
Enter the matrix and vector into \script, note the poor \verb|rcond|, and solve with \verb|x=A\b| to determine \(\sloppy\xv=(0.61,-0.64,-0.65,-0.93)\).
\setbox\ajrqrbox\hbox{\qrcode{% 4x4 approx Ax=b
A=[-1.1 0.1 0.7 -0.1
  0.1 -0.1 1.2 -0.6
  0.8 -0.2 0.4 -0.8
  0.8 0.1 -2.0 1.0]
b=[-1.1;-0.1;1.1;0.8]
rcond(A)
x=A\slosh b
Ad=A+(rand(4)*9-4.5)/100 
x=Ad\slosh b
[U,S,V]=svd(A)
z=U(:,1:3)'*b
y=z./diag(S(1:3,1:3))
x=V(:,1:3)*y
}}%
\marginajrbox%

To within experimental error the following matrices approximate~\(A\), and \script\ \verb|x=A\b| gives the corresponding solutions.
\begin{itemize}\small
\item \(A'=\begin{bmatrix} -1.10&0.11&0.67&-0.08
\\0.08&-0.10&1.17&-0.59
\\0.75&-0.21&0.39&-0.83
\\0.79&0.08&-1.96&0.98 \end{bmatrix}\),
\(\xv'=\begin{bmatrix} 0.64
\\-0.40
\\-0.64
\\-0.95 \end{bmatrix}\)
\item \(A''=\begin{bmatrix} -1.10&0.08&0.66&-0.14
\\0.08&-0.09&1.22&-0.58
\\0.77&-0.18&0.39&-0.78
\\0.75&0.11&-2.01&1.04 \end{bmatrix}\),
\(\xv''=\begin{bmatrix} 0.87
\\1.09
\\-0.58
\\-1.09 \end{bmatrix}\)
\end{itemize}
There are significant differences, mainly in the second component, between these equally valid solutions~\(\xv\), \(\xv'\) and~\(\xv''\).
The problem is again that, relative to the experimental error, there is a small singular value in the matrix~\(A\).
We must use an \svd\ to find all solutions consistent with the experimental error: compute \verb|[U,S,V]=svd(A)| to find~\twodp
\begin{verbatim}
U =
  -0.33   0.59  -0.36   0.64
  -0.43  -0.31   0.71   0.46
  -0.16  -0.74  -0.59   0.27
   0.82  -0.07   0.12   0.55
S =
   2.89      0      0      0
      0   1.50      0      0
      0      0   0.26      0
      0      0      0   0.02
V =
   0.30  -0.88   0.35   0.09
   0.04   0.15   0.09   0.98
  -0.85  -0.08   0.52   0.00
   0.43   0.43   0.78  -0.16
\end{verbatim}
The singular value~\(0.02\) is less than the error~\(\pm0.05\) so is effectively zero.
Hence solve the system as if this singular value is zero; that is, as if matrix~\(A\) has rank three.
Compute the smallest consistent solution with 
\begin{verbatim}
  z=U(:,1:3)'*b
  y=z./diag(S(1:3,1:3))
  x=V(:,1:3)*y
\end{verbatim}
Then add an arbitrary multiple of the last column of~\verb|V| to determine a general solution~\twodp
\begin{equation*}
\xv=(0.65,-0.22,-0.65,-1.00)+t(0.09,0.98,0,-0.16).
\end{equation*}
That the second component of \((0.09,0.98,0,-0.16)\) is the largest corresponds to the second component in each of~\xv, \(\xv'\) and~\(\xv''\) being the most sensitive, as seen in the above three cases.
\end{solution}
\end{example}
\end{reduce}


Most often the \idx{singular value}s are spread over a wide range of orders of magnitude.
In such cases an assessment of the errors in the matrix is crucial in what one reports as a solution.
The following artificial example illustrates the range.

\begin{example}[various errors] \label{eg:hilb5}
The matrix 
\begin{equation*}
A=\begin{bmatrix} 1&\frac12&\frac13&\frac14&\frac15
\\\frac12&\frac13&\frac14&\frac15&\frac16
\\\frac13&\frac14&\frac15&\frac16&\frac17
\\\frac14&\frac15&\frac16&\frac17&\frac18
\\\frac15&\frac16&\frac17&\frac18&\frac19
\end{bmatrix}
\end{equation*}
is an example of a so-called \idx{Hilbert matrix}.
Explore the effects of various assumptions about possible errors in~\(A\) upon the solution to \(A\xv=\vec 1\) where \(\vec1:=(1,1,1,1,1)\).
\begin{solution} 
Enter the matrix~\(A\) into \script\ with \index{hilb()@\texttt{hilb()}}\verb|A=hilb(5)| for the above \(5\times5\) Hilbert matrix, and enter the right-hand side with \verb|b=ones(5,1)|.
\begin{itemize}
\item First assume there is insignificant error in~\(A\) (there is always the base error of~\(10^{-15}\) in computation).
Then \cref{pro:unisol} finds that although the reciprocal of the condition number \(\verb|rcond(A)|\approx10^{-6}\) is bad, the \idx{unique solution} to \(A\xv=\vec1\)\,, obtained via \verb|x=A\b|, is
\begin{equation*}
\xv=(5,-120,630,-1120,630).
\end{equation*}
 
\setbox\ajrqrbox\hbox{\qrcode{% approx hilb
A=hilb(5)
b=ones(5,1)
rcond(A)
x=A\slosh b
[U,S,V]=svd(A)
z=U'*b
y=z./diag(S)
k=4
x=V(:,1:k)*y(1:k)
}}%
\marginajrbox%

\item Second suppose the errors in~\(A\) are roughly~\(10^{-5}\).
This level of error is a concern as \(\verb|rcond|\approx10^{-6}\) so errors would be magnified by~\(10^6\) in a direct solution of \(A\xv=\vec1\) (\cref{thm:erramp}).
Here we explore when all errors are in~\(A\) and none in the right-hand side vector~\(\vec 1\).
To explore, adopt \cref{pro:appmat}.
\begin{enumerate}
\item Find an \svd\ \(A=\usv\) via \verb|[U,S,V]=svd(A)| \twodp
\begin{verbatim}
U =
  -0.77   0.60  -0.21   0.05   0.01
  -0.45  -0.28   0.72  -0.43  -0.12
  -0.32  -0.42   0.12   0.67   0.51
  -0.25  -0.44  -0.31   0.23  -0.77
  -0.21  -0.43  -0.57  -0.56   0.38
S =
   1.57      0      0      0      0
      0   0.21      0      0      0
      0      0   0.01      0      0
      0      0      0   0.00      0
      0      0      0      0   0.00
V =
  -0.77   0.60  -0.21   0.05   0.01
  -0.45  -0.28   0.72  -0.43  -0.12
  -0.32  -0.42   0.12   0.67   0.51
  -0.25  -0.44  -0.31   0.23  -0.77
  -0.21  -0.43  -0.57  -0.56   0.38
\end{verbatim}
More informatively, the singular values have the following wide range of magnitudes, 
\begin{eqnarray*}
&&\sigma_1=1.57,\quad
\sigma_2=0.21,\quad
\sigma_3=1.14\cdot10^{-2},\quad
\\&&
\sigma_4=3.06\cdot10^{-4},\quad
\sigma_5=3.29\cdot10^{-6}.
\end{eqnarray*}
\item Because the assumed error~\(10^{-5}\) lies between~\(\sigma_4\) and~\(\sigma_5\), \(\sigma_4>10^{-5}>\sigma_5\), the matrix~\(A\) is effectively of rank four, \(k=4\)\,.
\item Solving the system \(A\xv=\usv\xv=\vec1\) as rank four, in the least square sense, \cref{pro:appsol} gives \twodp
\begin{enumerate}
\item \(\zv=\tr U\vec1=(-2.00,-0.97,-0.24,-0.04,0.00)\),
\item neglect the fifth component of~\zv\ as an error and obtain the first four components of~\yv\ via \verb|y=z(1:4)./diag(S(1:4,1:4))| so that
\begin{equation*}
\yv=(-1.28,-4.66,-21.43,-139.69,y_5),
\end{equation*}

\item then the smallest, least square, solution determined with 
\verb|x=V(:,1:4)*y| is
\begin{equation*}
\xv=(-3.82,46.78,-93.41,-23.53,92.27),
\end{equation*}
and a general solution includes the arbitrary multiple~\(y_5\) of the last column of~\(V\) to be
\begin{eqnarray*}
\xv&=&(-3.82,46.78,-93.41,-23.53,92.27)
\\&&{}+y_5(0.01,-0.12,0.51,-0.77,0.38).
\end{eqnarray*}
\end{enumerate}
\end{enumerate}

\item Third suppose the errors in~\(A\) are roughly~\(10^{-3}\).
Re-adopt \cref{pro:appmat}.
\begin{enumerate}
\item Use the same \svd, \(A=\usv\).
\item Because the assumed error~\(10^{-3}\) satisfies \(\sigma_3>10^{-3}>\sigma_4\) the matrix~\(A\) is effectively of rank three, \(k=3\)\,.
\item Solving the system \(A\xv=\usv\xv=\vec1\) as rank three, in the least square sense, \cref{pro:appsol} gives \twodp\ the same~\zv, and the same first three components in
\begin{equation*}
\yv=(-1.28,-4.66,-21.43,y_4,y_5),
\end{equation*}
then \verb|x=V(:,1:3)*y| determines the smallest, least square, solution
\begin{equation*}
\xv=(2.76,-13.66,-0.19,9.03,14.38),
\end{equation*}
and a general solution includes the arbitrary multiples of the last columns of~\(V\) to be
\begin{eqnarray*}
\xv&=&(2.76,-13.66,-0.19,9.03,14.38)
\\&&{}+y_4(0.05,-0.43,0.67,0.23,-0.56)
\\&&{}+y_5(0.01,-0.12,0.51,-0.77,0.38).
\end{eqnarray*}
\end{enumerate}

\item Lastly suppose the errors in~\(A\) are roughly~\(0.05\).
Re-adopt \cref{pro:appmat}.
\begin{enumerate}
\item Use the same \svd, \(A=\usv\).
\item Because the assumed error~\(0.05\) satisfies \(\sigma_2>0.05>\sigma_3\) the matrix~\(A\) is effectively of rank two, \(k=2\)\,.
\item Solving the system \(A\xv=\usv\xv=\vec1\) as rank two, in the least square sense, \cref{pro:appsol} gives \twodp\ the same~\zv, and the same first two components in
\begin{equation*}
\yv=(-1.28,-4.66,y_3,y_4,y_5),
\end{equation*}
then \verb|x=V(:,1:2)*y| determines the smallest, least square, solution
\begin{equation*}
\xv=(-1.83,1.85,2.39,2.39,2.27),
\end{equation*}
and a general solution includes the arbitrary multiples of the last columns of~\(V\) to be
\begin{eqnarray*}
\xv&=&(-1.83,1.85,2.39,2.39,2.27)
\\&&{}+y_3(-0.21,0.72,0.12,-0.31,-0.57)
\\&&{}+y_4(0.05,-0.43,0.67,0.23,-0.56)
\\&&{}+y_5(0.01,-0.12,0.51,-0.77,0.38).
\end{eqnarray*}
\end{enumerate}
\end{itemize}
The level of error makes a major difference in the qualitative nature of allowable solutions: here from a unique solution through to a three parameter family of equally valid solutions.
To appropriately solve systems of linear equations we must know the level of error.
\end{solution}
\end{example}




\begin{example}[translating temperatures] 
Recall \cref{eg:infertemp2} attempts to fit a quartic polynomial to observations (plotted in the margin) of the relation between Celsius and Fahrenheit temperature. 
The attempt failed because \verb|rcond| is too small.
Let's try again now that we can cater for matrices with errors.
Recall the data between temperatures reported by a European and an American are the following:
\marginpar{\begin{tikzpicture}
\begin{axis}[footnotesize,axis lines=middle
    ,xlabel={European, $T_E$},ylabel={American, $T_A$}
    ,xmin=5,xmax=35,ymin=41,ymax=95]
    \addplot+[only marks] coordinates {
    (26,80) (15,60) (11,51) (23,74) (27,81) };
\end{axis}
\end{tikzpicture}}%
\begin{equation*}
\begin{array}{l|rrrrr}
T_E&15&26&11&23&27\\
T_A&60&80&51&74&81
\end{array}
\end{equation*}
\cref{eg:infertemp2} attempts to fit the data with the quartic polynomial
\begin{equation*}
T_A=c_1+c_2T_E+c_3T_E^2+c_4T_E^3+c_5T_E^4,
\end{equation*}
and deduced the following system of equations for the coefficients
\begin{equation*}
\begin{bmatrix} 1&15&225&3375&50625
\\1&26&676&17576&456976
\\1&11&121&1331&14641
\\1&23&529&12167&279841
\\1&27&729&19683&531441 \end{bmatrix}
\begin{bmatrix} c_1\\c_2\\c_3\\c_3\\c_4\\c_5 \end{bmatrix}
=\begin{bmatrix} 60\\80\\51\\74\\81 \end{bmatrix}.
\end{equation*}
In order to find a robust solution, here let's approximate both the matrix and the right-hand side vector because both the matrix and the vector come from real data with errors of about up to~\(\pm0.5^\circ\).

\begin{solution}
Now invoke \cref{pro:appmat} to approximate the system of linear equations, and solve the approximate problem.
\begin{enumerate}
\item
There is a problem in approximating the matrix: the columns are of wildly different \idx{magnitude}s.  
In contrast, our mathematical analysis treats all columns the same.  
The problem is that each column comes from different powers of temperatures.  
To avoid the problem we must \index{data scaling}scale the temperature data for the matrix.  
The simplest scaling is to divide by a typical temperature.
That is, instead of seeking a fit in terms of powers of~\(T_E\), we seek a fit in powers of~\(T_E/20^\circ\) as \(20\)~degrees is a typical temperature in the data.
Hence let's here fit the data with the quartic polynomial
\begin{equation*}
T_A=c_1+c_2\frac{T_E}{20} +c_3\left(\frac{T_E}{20}\right)^2+c_4\left(\frac{T_E}{20}\right)^3+c_5\left(\frac{T_E}{20}\right)^4,
\end{equation*}
which gives the following system for the coefficients \twodp
\begin{equation*}
\begin{bmatrix} 1&0.75&0.56&0.42&0.32
\\1&1.30&1.69&2.20&2.86
\\1&0.55&0.30&0.17&0.09
\\1&1.15&1.32&1.52&1.75
\\1&1.35&1.82&2.46&3.32 \end{bmatrix}
\begin{bmatrix} c_1\\c_2\\c_3\\c_3\\c_4\\c_5 \end{bmatrix}
=\begin{bmatrix} 60\\80\\51\\74\\81 \end{bmatrix}.
\end{equation*}
Now all the components of the matrix are roughly the same magnitude, as required.

There is no need to \index{data scaling}scale the right-hand side vector as all components are all roughly the same magnitude, they are all simply `American temperatures'. 

In script\ construct the scaled matrix and right-hand side vector with 
\begin{verbatim}
te=[15;26;11;23;27]
ta=[60;80;51;74;81]
tes=te/20
A=[ones(5,1) tes tes.^2 tes.^3 tes.^4]
\end{verbatim}
\setbox\ajrqrbox\hbox{\qrcode{% approx problem
te=[15;26;11;23;27]
ta=[60;80;51;74;81]
tes=te/20
A=[ones(5,1) tes tes.^2 tes.^3 tes.^4]
[U,S,V]=svd(A)
z=U'*ta
y=z(1:3)./diag(S(1:3,1:3))
c3=V(:,1:3)*y
}}%
\marginajrbox%

\item Compute an \svd, \(A=\usv\), with \verb|[U,S,V]=svd(A)| to get \twodp
\begin{verbatim}
U =
  -0.16   0.64   0.20   0.72  -0.12
  -0.59  -0.13  -0.00  -0.15  -0.78
  -0.10   0.67  -0.59  -0.45   0.05
  -0.42   0.23   0.68  -0.42   0.36
  -0.66  -0.28  -0.39   0.29   0.49
S =
   7.26      0      0      0      0
      0   1.44      0      0      0
      0      0   0.21      0      0
      0      0      0   0.02      0
      0      0      0      0   0.00
V =
  -0.27   0.78  -0.49  -0.27   0.09
  -0.32   0.39   0.36   0.66  -0.42
  -0.40   0.09   0.55  -0.09   0.72
  -0.50  -0.17   0.25  -0.62  -0.52
  -0.65  -0.44  -0.51   0.32   0.14
\end{verbatim}

\item Now choose the effective rank of the matrix to be the number of singular values bigger than the error.
Here recall that the temperatures in the matrix have been divided by~\(20^\circ\).
Hence the errors of roughly~\(\pm0.5^\circ\) in each temperature becomes roughly \(\pm0.5/20=\pm0.025\) in the scaled components in the matrix.%
\footnote{Discerning people may comment that raising to a power also amplifies errors so that the \(j\)th~column has errors more like~\(j\cdot0.5/20\).  However, let's ignore this extra complication here.}
There are three singular values larger than the error~\(0.025\), so the matrix effectively has rank three.
The two singular values less than the error~\(0.025\) are effectively zero.
That is, although it is not necessary to construct, we approximate the matrix~\(A\) by \twodp
\begin{equation*}
A_3=US_3\tr V=\begin{bmatrix} 1&0.74&0.56&0.43&0.31
\\1&1.30&1.69&2.20&2.86
\\1&0.56&0.30&0.16&0.09
\\1&1.16&1.32&1.52&1.75
\\1&1.35&1.82&2.46&3.32 \end{bmatrix}:
\end{equation*}
the differences between this approximate~\(A_3\) and the original~\(A\) are only~\(\pm0.01\), so matrix~\(A_3\) is indeed close to~\(A\).

\item Solve the equations as if matrix~\(A\) has rank three.
\begin{enumerate}
\item Find \(\zv=U'\bv\) via \verb|z=U'*ta| to find
\begin{verbatim}
z =
  -146.53
    56.26
     0.41
     1.06
    -0.69
\end{verbatim}
As matrix~\(A\) has effective rank of three, we approximate the right-hand side data by neglecting the last two components in this~\zv.
That the last two components in~\zv\ are small compared to the others indicates this neglect is a reasonable approximation.

\item Find \yv\ by solving \(S\yv=\zv\) as a rank three system via the command \verb|y=z(1:3)./diag(S(1:3,1:3))| to find  \twodp
\begin{equation*}
\yv=(-20.19,39.15,1.95,y_4,y_5).
\end{equation*}
The \idx{smallest solution} would be obtained by setting \(y_4=y_5=0\)\,.

\item Finally determine the coefficients \(\cv=V\yv\) with command \verb|c=V(:,1:3)*y| and then add arbitrary multiples of the remaining columns of~\(V\) to obtain the general solution \twodp
\begin{equation*}
\cv=
\begin{bmatrix} 35.09 \\22.50 \\12.77 \\3.99 \\-5.28 \end{bmatrix}
+y_4\begin{bmatrix} -0.27
\\0.66
\\-0.09
\\-0.62
\\0.32 \end{bmatrix}
+y_5\begin{bmatrix} 0.09
\\-0.42
\\0.72
\\-0.52
\\0.14 \end{bmatrix}.
\end{equation*}
\end{enumerate}

\item Obtain the solution with smallest coefficients by setting \(y_4=y_5=0\)\,.
This would fit the data with the quartic polynomial
\begin{eqnarray*}
T_A&=&35.09+22.50\frac{T_E}{20} 
+12.77\left(\frac{T_E}{20}\right)^2
\\&&{}
+3.99\left(\frac{T_E}{20}\right)^3
-5.28\left(\frac{T_E}{20}\right)^4.
\end{eqnarray*}%
But choosing the polynomial with smallest coefficients has little meaning in this application.
Surely we prefer a polynomial with fewer terms, fewer nonzero coefficients.
Surely we would prefer, say, the quadratic (plotted in the margin)
\marginpar{\begin{tikzpicture}
\begin{axis}[footnotesize,axis lines=middle
    ,xlabel={European, $T_E$},ylabel={American, $T_A$}
    ,xmin=5,xmax=35,ymin=41,ymax=94 ]
    \addplot+[only marks] coordinates {
    (26,80) (15,60) (11,51) (23,74) (27,81) };
%    \addplot+[domain=6:33,no marks] { 35.09+22.50*(x/20)+12.77*(x/20)^2+3.99*(x/20)^3-5.28*(x/20)^4 };
    \addplot+[domain=6:33,no marks] { 25.93+49.63*(x/20)-6.45*(x/20)^2 };
\end{axis}
\end{tikzpicture}}
\begin{equation*}
T_A=25.93+49.63\frac{T_E}{20} 
-6.45\left(\frac{T_E}{20}\right)^2.
\end{equation*}

In this application, let's use the freedom in~\(y_4\) and~\(y_5\) to set two of the coefficients to zero in the quartic.
Since \(c_4=3.99\) and \(c_5=-5.28\) are the smallest coefficients, and because they correspond to the highest powers in the quartic, it is natural to choose to make them both zero.
Let's redo the last step in the procedure.%
\footnote{Alternatively, one could redo the linear algebra to seek a quadratic from the outset rather than a quartic.  
The two alternative answers for a quadratic are not the same, but they are nearly the same.  
The small differences in the answers are because one approach modifies the matrix by recognizing its errors, and the other approach only modifies the right-hand side vector.}

The last step in the procedure is to solve \(\tr V\cv=\yv\) for~\cv. With the last two components of~\cv\ set to zero, from the computed \svd\ this is the system of equations \twodp
\begin{equation*}
\begin{bmatrix} -0.27&-0.32&-0.40&-0.50&-0.65
\\0.78&0.39&0.09&-0.17&-0.44
\\-0.49&0.36&0.55&0.25&-0.51
\\-0.27&0.66&-0.09&-0.62&0.32
\\0.09&-0.42&0.72&-0.52&0.14 \end{bmatrix}
\begin{bmatrix} c_1\\c_2\\c_3\\0\\0 \end{bmatrix}
=\begin{bmatrix} -20.19\\39.15\\1.95\\y_4\\y_5 \end{bmatrix},
\end{equation*}
where \(y_4\) and~\(y_5\) can be anything for equally good solutions.
Considering only the first three rows of this system, and using the zeros in~\cv, this system becomes
\begin{equation*}
\begin{bmatrix} -0.27&-0.32&-0.40
\\0.78&0.39&0.09
\\-0.49&0.36&0.55 \end{bmatrix}
\begin{bmatrix} c_1\\c_2\\c_3 \end{bmatrix}
=\begin{bmatrix} -20.19\\39.15\\1.95 \end{bmatrix}.
\end{equation*}
This is a basic system of three equations for three unknowns.
Since the matrix is the first three rows and columns of~\(\tr V\) and the right-hand side is the three components of~\yv\ already computed,  we solve the equation by 
\begin{itemize}
\item  checking the \idx{condition number}, \verb|rcond(V(1:3,1:3))| is~\(0.05\) which is good, and
\item then \verb|c=V(1:3,1:3)'\y| determines the coefficients \((c_1,c_2,c_3)=(25.93,49.63,-6.45)\) \twodp.
\end{itemize}
That is, a just as good polynomial fit, consistent with errors in the data, is the simpler quadratic polynomial (as plotted previously)
\begin{equation*}
T_A=25.93+49.63\frac{T_E}{20} 
-6.45\left(\frac{T_E}{20}\right)^2.
\end{equation*}
\end{enumerate}
\end{solution}
\end{example}



\begin{quoted}{\index{Punch, John}John Punch (1639)}
\idx{Occam's razor}: Non sunt multiplicanda entia sine necessitate
[Entities must not be multiplied beyond necessity]
\end{quoted}
\begin{comment}
``We consider it a good principle to explain the phenomena by the simplest hypothesis possible." Ptolemy (c.\textsc{ad}90--168)
\end{comment}


%\begin{example}  
%Recall \cref{eg:gps2,eg:gps3t} showed how linear equations arise in using the \idx{Global Positioning System} to determine locations.
%Let's revisit the problem given that \gps\ data from the satellites has errors.
%\begin{comment}
%Satellite location error is typically a few metres (called ephemeris error) and satellite clock error also leads to a couple of metres error.
%Most error is in determining the travel time due to atmospheric effects and measurement errors.
%Net effect should be error of about 15\,m.
%\url{http://en.wikipedia.org/wiki/Error_analysis_for_the_Global_Positioning_System}
%Not really suitable here as measurements are too precise: better as example for over-determined system and no errors.
%\end{comment}
%\end{example}




\begin{example} \label{eg:ctscan3x3d2}
Recall that \cref{ex:ctscan3x3d} introduced extra `diagonal' measurements into a 2D \index{CT scan}\textsc{ct}-scan.
As shown in the margin, the 2D region is divided into a \(3\times3\) grid of nine blocks.
\marginpar{\begin{tikzpicture} 
\begin{axis}[small,font=\footnotesize
,axis equal image,axis lines=none
]
  \addplot[] coordinates {(0,0)(3,0)(3,1)(0,1)(0,2)(3,2)(3,3)(0,3)
  (0,0)(1,0)(1,3)(2,3)(2,0)(3,0)(3,3)};
  \node at (axis cs:0.5,0.5) {\large$r_3$};
  \node at (axis cs:1.5,0.5) {\large$r_6$};
  \node at (axis cs:2.5,0.5) {\large$r_9$};
  \node at (axis cs:0.5,1.5) {\large$r_2$};
  \node at (axis cs:1.5,1.5) {\large$r_5$};
  \node at (axis cs:2.5,1.5) {\large$r_8$};
  \node at (axis cs:0.5,2.5) {\large$r_1$};
  \node at (axis cs:1.5,2.5) {\large$r_4$};
  \node at (axis cs:2.5,2.5) {\large$r_7$};
  \addplot[blue,quiver={u=4,v=0},-stealth] coordinates {(-0.5,0.5)(-0.5,1.5)(-0.5,2.5)};
  \addplot[blue,quiver={u=0,v=4},-stealth] coordinates {(0.5,-0.5)(1.5,-0.5)(2.5,-0.5)};
  \addplot[blue,quiver={u=-3.2,v=-3.2},-stealth] coordinates {(3,3)(2.5,3.5)(3.5,2.5)};
  \node[right] at (axis cs:0.5,3.5) {$f_1$};
  \node[right] at (axis cs:1.5,3.5) {$f_2$};
  \node[right] at (axis cs:2.5,3.5) {$f_3$};
  \node[above] at (axis cs:3.5,0.5) {$f_6$};
  \node[above] at (axis cs:3.5,1.5) {$f_5$};
  \node[above] at (axis cs:3.5,2.5) {$f_4$};
  \node[left] at (axis cs:-0.2,-0.2) {$f_8$};
  \node[left] at (axis cs:-0.7,+0.3) {$f_7$};
  \node[left] at (axis cs:+0.3,-0.7) {$f_9$};
\end{axis}
\end{tikzpicture}}%
Then measurements are taken of the \idx{X-ray}s not absorbed along the shown nine paths: three horizontal, three vertical, and three diagonal.
Suppose the measured fractions of X-ray energy are \(\sloppy\fv=(0.048, 0.081, 0.042, 0.020, 0.106, 0.075, 0.177, 0.181, 0.105)\).
Use an \svd\ to find the `greyest' transmission factors \idx{consistent} with the measurements and likely errors.
%\begin{verbatim}
%A=[1 1 1 0 0 0 0 0 0 
% 0 0 0 1 1 1 0 0 0 
% 0 0 0 0 0 0 1 1 1
% 1 0 0 1 0 0 1 0 0 
% 0 1 0 0 1 0 0 1 0 
% 0 0 1 0 0 1 0 0 1 
% 0 1 0 1 0 0 0 0 0
% 0 0 1 0 1 0 1 0 0
% 0 0 0 0 0 1 0 1 0]
%b=log([0.048
% 0.081
% 0.042
% 0.020
% 0.106
% 0.075
% 0.177
% 0.181
% 0.105])
%[U,S,V]=svd(A)
%z=U'*b
%y=z(1:7)./diag(S(1:7,1:7))
%x=V(:,1:7)*y
%r=reshape(exp(A\b),3,3)
%r7=reshape(exp(x),3,3)
%\end{verbatim}

\begin{solution} 
Nine X-ray measurements are made through the body where \hlist f9\ denote the fraction of energy in the measurements relative to the power of the X-ray beam.
Thus we need to solve nine equations for the nine unknown transmission factors:
\begin{eqnarray*}
&&
r_1r_2r_3=f_1\,,\quad
r_4r_5r_6=f_2\,,\quad
r_7r_8r_9=f_3\,,\quad
\\&&
r_1r_4r_7=f_4\,,\quad
r_2r_5r_8=f_5\,,\quad
r_3r_6r_9=f_6\,,\quad
\\&&
r_2r_4=f_7\,,\qquad
r_3r_5r_7=f_8\,,\quad
r_6r_8=f_9\,.\quad
\end{eqnarray*}
Turn such \idx{nonlinear equation}s into linear equations by taking the 
\idx{logarithm} (to any base, but here say the \index{log}\idx{natural logarithm} to base~\(e\)) of both sides of all equations:
\begin{equation*}
r_ir_jr_k=f_l \iff (\ln r_i)+(\ln r_j)+(\ln r_k)=(\ln f_l).
\end{equation*}
That is, letting new unknowns \(x_i=\ln r_i\) and new right-hand sides \(b_i=\ln f_i\)\,, we aim to solve a system of nine linear equations for nine unknowns:
\begin{eqnarray*}&&
x_1+x_2+x_3=b_1\,,\quad
x_4+x_5+x_6=b_2\,,\quad
x_7+x_8+x_9=b_3\,,
\\&&
x_1+x_4+x_7=b_4\,,\quad
x_2+x_5+x_8=b_5\,,\quad
x_3+x_6+x_9=b_6\,,\quad
\\&&
x_2+x_4\phantom{{}+x_4}=b_7\,,\quad
x_3+x_5+x_7=b_8\,,\quad
x_6+x_8\phantom{{}+x_4}=b_9\,.
\end{eqnarray*}
These form the matrix-vector system \(A\xv=\bv\) for \(9\times9\) matrix
\begin{equation*}
A=\begin{bmatrix} 
 1&1&1&0&0&0&0&0&0 \\
 0&0&0&1&1&1&0&0&0 \\
 0&0&0&0&0&0&1&1&1 \\
 1&0&0&1&0&0&1&0&0 \\
 0&1&0&0&1&0&0&1&0 \\
 0&0&1&0&0&1&0&0&1 \\
 0&1&0&1&0&0&0&0&0 \\
 0&0&1&0&1&0&1&0&0 \\
 0&0&0&0&0&1&0&1&0 \end{bmatrix},\quad
 \bv=\ln\begin{bmatrix} .048
\\ .081
\\ .042
\\ .020
\\ .106
\\ .075
\\ .177
\\ .181
\\ .105 \end{bmatrix}
=\begin{bmatrix} -3.04
\\ -2.51
\\ -3.17
\\ -3.91
\\ -2.24
\\ -2.59
\\ -1.73
\\ -1.71
\\ -2.25 \end{bmatrix}.
\end{equation*}
\setbox\ajrqrbox\hbox{\qrcode{% ctscan factors
A=[1 1 1 0 0 0 0 0 0 
 0 0 0 1 1 1 0 0 0 
 0 0 0 0 0 0 1 1 1
 1 0 0 1 0 0 1 0 0 
 0 1 0 0 1 0 0 1 0 
 0 0 1 0 0 1 0 0 1 
 0 1 0 1 0 0 0 0 0
 0 0 1 0 1 0 1 0 0
 0 0 0 0 0 1 0 1 0]
b=log([0.048
 0.081
 0.042
 0.020
 0.106
 0.075
 0.177
 0.181
 0.105])
}}%
\marginajrbox%
Implement \cref{pro:appmat}.
\begin{enumerate}
\item Here there is no need to \index{data scaling}scale the vector~\bv\ as all entries are roughly the same.
There is no need to scale the matrix~\(A\) as all entries mean the same, namely simply zero or one depending upon whether  beam passes through the a pixel square.
However, the entries of~\(A\) are in error in two ways.
\begin{itemize}
\item A diagonal beam has a path through a pixel that is up to 41\% longer than horizontal or vertical beam--which is not accounted for.  Further, a beam has finite width so it also passes through part of some off-diagonal pixels---which is not represented.
\item Similarly, a horizontal or vertical beam has finite width and may underrepresent the sides of the pixels it goes through, and/or involve parts of neighbouring pixels---neither effect is represented.
\end{itemize}
Consequently the entries in the matrix~\(A\) could easily have error of~\(\pm0.5\).
Let's use knowledge of this error to ensure the predictions by the scan are reliable.

\item Compute an \svd, \(A=\usv\), via \verb|[U,S,V]=svd(A)| \twodp
{\small
\begin{verbatim}
U =
  0.33 -0.41  0.27  0.21  0.54  0.23 -0.29  0.13 -0.41
  0.38 -0.00 -0.47  0.36 -0.45 -0.19 -0.00  0.32 -0.41
  0.33  0.41  0.27 -0.57 -0.08  0.23  0.29  0.13 -0.41
  0.33 -0.41  0.27 -0.21 -0.54  0.23 -0.29  0.13  0.41
  0.38 -0.00 -0.47 -0.36  0.45 -0.19 -0.00  0.32  0.41
  0.33  0.41  0.27  0.57  0.08  0.23  0.29  0.13  0.41
  0.23 -0.41 -0.29 -0.00  0.00  0.30  0.58 -0.52 -0.00
  0.41  0.00  0.33  0.00  0.00 -0.74 -0.00 -0.43  0.00
  0.23  0.41 -0.29 -0.00 -0.00  0.30 -0.58 -0.52  0.00
S =
  2.84     0     0     0     0     0     0     0     0
     0  2.00     0     0     0     0     0     0     0
     0     0  1.84     0     0     0     0     0     0
     0     0     0  1.73     0     0     0     0     0
     0     0     0     0  1.73     0     0     0     0
     0     0     0     0     0  1.51     0     0     0
     0     0     0     0     0     0  1.00     0     0
     0     0     0     0     0     0     0  0.51     0
     0     0     0     0     0     0     0     0  0.00
V =
  0.23 -0.41  0.29  0.00  0.00  0.30 -0.58  0.52 -0.00
  0.33 -0.41 -0.27 -0.08  0.57  0.23  0.29 -0.13 -0.41
  0.38  0.00  0.47  0.45  0.36 -0.19 -0.00 -0.32  0.41
  0.33 -0.41 -0.27  0.08 -0.57  0.23  0.29 -0.13  0.41
  0.41 -0.00 -0.33  0.00 -0.00 -0.74 -0.00  0.43 -0.00
  0.33  0.41 -0.27  0.54 -0.21  0.23 -0.29 -0.13 -0.41
  0.38  0.00  0.47 -0.45 -0.36 -0.19  0.00 -0.32 -0.41
  0.33  0.41 -0.27 -0.54  0.21  0.23 -0.29 -0.13  0.41
  0.23  0.41  0.29  0.00  0.00  0.30  0.58  0.52  0.00
\end{verbatim}
}%

\item Here choose the rank of the matrix to be effectively seven as two of the nine singular values, namely \(0.51\) and~\(0.00\), are less than or about the magnitude of the expected error, roughly~\(0.5\).

\item Use the rank seven \svd\ to solve the approximate system as in \cref{pro:appsol}.
\begin{enumerate}
\item Find \(\zv=\tr U\bv\) via \verb|z=U'*b| to find
\begin{verbatim}
z =
  -7.63
   0.27
  -0.57
   0.42
   0.64
  -1.95
   0.64
  -0.40
  -0.01
\end{verbatim}

\item Neglect the last two rows in solving \(S_7\yv=\zv\) to find via
\verb|y=z(1:7)./diag(S(1:7,1:7))| that the first seven components of~\yv\ are
\begin{verbatim}
y =
  -2.69
   0.14
  -0.31
   0.24
   0.37
  -1.29
   0.64
\end{verbatim}
The last two components of~\yv, \(y_8\) and~\(y_9\), are free variables.

\item Obtain a \idx{particular solution} to \(\tr V\xv=\yv\), the one of smallest magnitude, by setting \(y_8=y_9=0\) and determining~\xv\ from \verb|x=V(:,1:7)*y| to get the smallest solution
\begin{verbatim}
x =
  -1.53
  -0.78
  -0.67
  -1.16
  -0.05
  -1.18
  -1.16
  -1.28
  -0.68
\end{verbatim}
Obtain other equally valid solutions, in the context of the identified error in matrix~\(A\), by adding arbitrary multiples of the last two columns of~\(V\).
\end{enumerate}

\item Here we aim to make predictions from the \textsc{ct}-scan\index{CT scan}. 
The `best' solution in this application is the one with least artificial features.
The smallest \idx{magnitude}~\xv\ seems to reasonably implement this criterion.
Thus use the above particular~\xv\ to determine the transmission factors, \(r_i=\exp(x_i)\).
\def\temp#1#2#3#4#5#6#7#8#9{\begin{tikzpicture}
\begin{axis}[tiny,axis equal image,colormap/blackwhite,axis lines=none]
\addplot[patch,patch type=rectangle
,point meta min={0},point meta max={1}
,table/row sep=\\,patch table with point meta={%
8 9 13 12   #1\\
4 5 9 8     #2\\
0 1 5 4     #3\\
9 10 14 13  #4\\
5 6 10 9    #5\\
1 2 6 5     #6\\
10 11 15 14 #7\\
6 7 11 10   #8\\
2 3 7 6     #9\\
}]
table[row sep=\\] {
x y \\
0 0\\% 0
1 0\\% 1
2 0\\% 2
3 0\\% 3
0 1\\% 4
1 1\\% 5
2 1\\% 6
3 1\\% 7
0 2\\% 8
1 2\\% 9
2 2\\% 10
3 2\\% 11
0 3\\% 12
1 3\\% 13
2 3\\% 14
3 3\\% 15
};
\end{axis}
\end{tikzpicture}}%end-def
Here use \verb|r=reshape(exp(x),3,3)|\index{reshape()@\texttt{reshape()}} to compute and form into the \(3\times3\) array of pixels
\begin{equation*}
\begin{array}{|r|r|r|}
\hline 0.22&0.31&0.31
\\\hline 0.46&0.95&0.28
\\\hline 0.51&0.31&0.51
\\\hline
\end{array}
\quad\mapsto\quad
\begin{array}{c}
\temp{0.22}{0.46}{0.51}{0.31}{0.95}{0.31}{0.31}{0.28}{0.51}\end{array}
\end{equation*}
when illustrated  with \index{colormap()@\texttt{colormap()}}\verb|colormap(gray),imagesc(r)|
\end{enumerate}
The \textsc{ct}-scan and linear algebra robustly identifies that there is a significant `hole' in the middle of the body being scanned.
\end{solution}
\end{example}




%\begin{comment}
%Maybe other application examples.
%\end{comment}










\subsection{Tikhonov regularization explained}

\index{Tikhonov regularization|(}

Regularization of poorly-posed linear equations is a widely invoked practical necessity.
\begin{aside} This optional subsection connects to much established practice that graduates may encounter.\end{aside}%
Many people have invented alternative techniques.
Many have independently re-invented techniques.
Perhaps the most common is the so-called \idx{Tikhonov regularization}.
This section introduces and discusses Tikhonov regularization.

\begin{quoted}{\idx{Wikipedia} (2015)}
In statistics, the method is known as ridge regression, and with multiple independent discoveries, it is also variously known as the Tikhonov--Miller method, the Phillips--Twomey method, the constrained linear inversion method, and the method of linear regularization.
\footnote{\protect\url{https://en.wikipedia.org/wiki/Tikhonov_regularization}}
\end{quoted}


\begin{definition} \label{def:Tikreg}
In seeking to solve the poorly-posed \idx{system} \(A\xv=\bv\) for \(m\times n\) matrix~\(A\), 
\begin{aside}
The greek letter~\(\alpha\) is `alpha' (different to the `proportional to' symbol~\(\propto\)).
\end{aside}%
a \bfidx{Tikhonov regularization} is the system \((\tr AA+\alpha^2I_n)\xv=\tr A\bv\) for some chosen \idx{regularization parameter} value~\(\alpha>0\).%
\footnote{Some may notice that a Tikhonov regularization is closely connected to the so-called \idx{normal equation} \((\tr AA)\xv=\tr A\bv\)\,.  
Tikhonov regularization shares with the \idx{normal equation} some practical limitations as well as some strengths.}
\end{definition}


\begin{example} 
Use \idx{Tikhonov regularization} to solve the system of \cref{eg:2regu}:
\begin{equation*}
0.5x+0.3y=1\quad\text{and}\quad 1.1x+0.7y=2\,,
\end{equation*}
\begin{solution} 
Here the matrix and right-hand side vector are
\begin{equation*}
A=\begin{bmatrix} 0.5&0.3\\1.1&0.7 \end{bmatrix},\quad
\bv=\begin{bmatrix} 1\\2 \end{bmatrix}.
\end{equation*}
Evaluating \(\tr AA\) and~\(\tr A\bv\), a \idx{Tikhonov regularization}, \((\tr AA+\alpha^2I_n)\xv=\tr A\bv\)\,, is then the system
\begin{equation*}
\begin{bmatrix} 1.46+\alpha^2&0.92\\0.92&0.58+\alpha^2 \end{bmatrix}\xv=\begin{bmatrix} 2.7\\1.7 \end{bmatrix}.
\end{equation*}
Choose regularization parameter~\(\alpha\) to be roughly the error: here the error is~\(\pm0.05\) so let's choose \(\alpha=0.1\) (\(\alpha^2=0.01\)).
Enter into \script\ with
\begin{verbatim}
A=[0.5 0.3;1.1 0.7]
b=[1.0;2.0]
AtA=A'*A+0.01*eye(2)
rcondAtA=rcond(AtA)
x=AtA\(A'*b)
\end{verbatim}
\setbox\ajrqrbox\hbox{\qrcode{% 2x2 Tikhonov
A=[0.5 0.3;1.1 0.7]
b=[1.0;2.0]
AtA=A'*A+0.01*eye(2)
rcondAtA=rcond(AtA)
x=AtA\slosh (A'*b)
}}%
\marginajrbox%
to find the Tikhonov regularized solution is \(\xv=(1.39,0.72)\).%
\footnote{Interestingly, \(\texttt{rcond}=0.003\) for the Tikhonov system which is worse than \(\texttt{rcond}(A)\).  The regularization only works at all because pre-multiplying by~\(\tr A\) pushes both sides into the row space of~\(A\) (except for numerical error and the small~\(\alpha^2I\) factor).}
This solution is reasonably close to the smallest solution found by the \svd\ which is~\((1.32,0.83)\).
However, Tikhonov regularization gives no hint of the reasonable general solutions found by the \svd\ approach of \cref{eg:2regu}.

Change the regularization parameter to \(\alpha=0.01\) and \(\alpha=1\) and see that both of these choices degrade the Tikhonov solution.
\end{solution}
\end{example}




\begin{activity}
In the linear system for \(\xv=(x,y)\),
\begin{equation*}
4x-y=-4  \quad\text{and}\quad
-2x+y=3\,,
\end{equation*}
the \idx{coefficient}s on the left-hand side of each equation are in error by about~\(\pm0.3\).  
Tikhonov regularization should solve which one of the following systems?
\actposs{\(\begin{bmatrix} 20.1&-6\\-6&2.1 \end{bmatrix}\xv=\begin{bmatrix} -22\\7 \end{bmatrix}\)}
{\(\begin{bmatrix} 20.3&-6\\-6&2.3 \end{bmatrix}\xv=\begin{bmatrix} -22\\7 \end{bmatrix}\)}
{\(\begin{bmatrix} 18.1&-5\\-10&3.1 \end{bmatrix}\xv=\begin{bmatrix} -19\\11 \end{bmatrix}\)}
{\(\begin{bmatrix} 18.3&-5\\-10&3.3 \end{bmatrix}\xv=\begin{bmatrix} -19\\11 \end{bmatrix}\)}
\end{activity}



Do not apply \idx{Tikhonov regularization} blindly as it does introduce biases. 
The following example illustrates the bias.

\begin{example} 
Recall \cref{eg:fourwts} at the start of \cref{sec:mmctp} where  scales variously reported my weight in kg as~\(84.8\), \(84.1\), \(84.7\) and~\(84.4\)\,.  
To best estimate my weight~\(x\) we rewrote the problem in matrix-vector form
\begin{equation*}
Ax=\bv\,,\quad\text{namely }
\begin{bmatrix} 1\\1\\1\\1 \end{bmatrix}x
=\begin{bmatrix} 84.8\\84.1\\84.7\\84.4 \end{bmatrix}.
\end{equation*}
A Tikhonov regularization of this \idx{inconsistent system} is
\begin{equation*}
\left(\begin{bmatrix} 1&1&1&1 \end{bmatrix}\begin{bmatrix} 1\\1\\1\\1 \end{bmatrix}+\alpha^2\right)x
=\begin{bmatrix} 1&1&1&1 \end{bmatrix}\begin{bmatrix} 84.8\\84.1\\84.7\\84.4 \end{bmatrix}.
\end{equation*}
That is, \((4+\alpha^2)x=338\)\,kg with solution \(x=338/(4+\alpha^2)=84.5/(1+\alpha^2/4)\)\,kg.
Because of the division by \(1+\alpha^2/4\), this Tikhonov answer is biased as it is systematically below the \idx{average}~\(84.5\)\,kg.
For small Tikhonov parameter~\(\alpha\) the bias is small, but even so such a bias is unpleasant.
\end{example}



\begin{example} 
Use \idx{Tikhonov regularization} to solve \(A\xv=\bv\) for the matrix and vector of \cref{eg:3regmata}.
\begin{solution} 
Here the matrix and right-hand side vector are
\begin{equation*}
A=\begin{bmatrix} -0.2&-0.6&1.8
\\ 0.0&0.2&-0.4
\\ -0.3&0.7&0.3 \end{bmatrix}, \quad
\bv=\begin{bmatrix} -0.5
\\ 0.1
\\ -0.2
 \end{bmatrix}.
\end{equation*}
A \idx{Tikhonov regularization}, \((\tr AA+\alpha^2I_n)\xv=\tr A\bv\)\,, is then the system
\begin{equation*}
\begin{bmatrix} 0.13+\alpha^2&-0.09&-0.45
\\-0.09&0.89+\alpha^2 &-0.95
\\-0.45&-0.95&3.49+\alpha^2\end{bmatrix}\xv
=\begin{bmatrix} 0.16\\0.18\\-1.00 \end{bmatrix}.
\end{equation*}
Choose regularization parameter~\(\alpha\) to be roughly the error: here the error is~\(\pm0.05\) so let's choose \(\alpha=0.1\) (\(\alpha^2=0.01\)).
Enter into and solve with \script\ via
\begin{verbatim}
A=[-0.2 -0.6 1.8
   0.0 0.2 -0.4
  -0.3 0.7 0.3 ]
b=[-0.5;0.1;-0.2]
AtA=A'*A+0.01*eye(3)
rcondAtA=rcond(AtA)
x=AtA\(A'*b)
\end{verbatim}
\setbox\ajrqrbox\hbox{\qrcode{% 3x3 Tikhonov
A=[-0.2 -0.6 1.8
   0.0 0.2 -0.4
  -0.3 0.7 0.3 ]
b=[-0.5;0.1;-0.2]
AtA=A'*A+0.01*eye(3)
rcondAtA=rcond(AtA)
x=AtA\slosh (A'*b)
}}%
\marginajrbox%
which finds the Tikhonov regularized solution is \(\xv=(0.10,-0.11,-0.30)\).
To two decimal places this is the same as the smallest solution found by an \svd.
However, Tikhonov regularization gives no hint of the reasonable general solutions found by the \svd\ approach of \cref{eg:3regmata}.
\end{solution}
\end{example}



Although \cref{def:Tikreg} does not look like it, Tikhonov regularization relates directly to the \svd\ regularization of \cref{sec:svdir}.
The next theorem establishes the connection.



\begin{theorem}[Tikhonov regularization] \label{thm:Tikreg}
Solving \(A\xv=\bv\) by \idx{Tikhonov regularization}, with parameter~\(\alpha>0\), is equivalent to finding the \index{smallest solution}smallest, \idx{least square}, solution of the \idx{system} \(A'\xv=\bv\) where  
%\begin{itemize}
%\item 
the matrix~\(A'\) is obtained from~\(A\) by replacing each of its nonzero \idx{singular value}s~\(\sigma_i\) by \(\sigma'_i:=\sigma_i+\alpha^2/\sigma_i\)\,.
%, and
%\item the vector~\(\tilde\bv\) is the orthogonal projection of~\(\bv\) onto the column space~\AA\ of~\(A\),  \(\tilde\bv=\proj_\AA\bv\)\,.
%\end{itemize}
\end{theorem}

\begin{proof} 
Let's use an \svd\ to understand Tikhonov regularization.
Suppose \(m\times n\) matrix~\(A\) has \svd\ \(A=\usv\).
First, the left-hand side matrix in a Tikhonov regularization is
\begin{eqnarray*}
\tr AA+\alpha^2I_n
&=&\tr{(\usv)}\usv+\alpha^2I_nV\tr V
\\&=&V\tr S\tr U\usv+\alpha^2VI_n\tr V
\\&=&V\tr SS\tr V+V(\alpha^2I_n)\tr V
\\&=&V(\tr SS+\alpha^2I_n)\tr V,
\end{eqnarray*}
whereas the right-hand side is 
\begin{equation*}
\tr A\bv =\tr{(\usv)}\bv =V\tr S\tr U\bv\,.
\end{equation*}
Corresponding to the variables used in previous procedures, let \(\zv=\tr U\bv\in\RR^m\) and as yet unknown \(\yv=\tr V\xv\in\RR^n\). 
Then equating the above two sides, and premultiplying by the orthogonal~\(\tr V\), means the Tikhonov regularization is equivalent to solving \((\tr SS+\alpha^2I_n)\yv=\tr S\zv\) for the as yet unknown~\yv.
(Beautifully, this equation could be interpreted as the Tikhonov regularization of the equation \(S\yv=\zv\)\,.)

Second, let \(r=\rank A\) so that the singular value matrix
\begin{equation*}
S=\begin{bmatrix} \begin{matrix} \sigma_1&\cdots&0\\
\vdots&\ddots&\vdots\\
0&\cdots&\sigma_r \end{matrix} & 
O_{r\times (n-r)}\\\,\\
O_{(m-r)\times r}&O_{(m-r)\times (n-r)}\end{bmatrix}
\end{equation*}
(where the bottom right zero block contains all the zero singular values).
Consequently, the equivalent Tikhonov regularization, \((\tr SS+\alpha^2I_n)\yv=\tr S\zv\), becomes
\begin{equation*}
\begin{bmatrix} \begin{matrix} \sigma_1^2+\alpha^2&\cdots&0\\
\vdots&\ddots&\vdots\\
0&\cdots&\sigma_r^2+\alpha^2 \end{matrix} & 
O_{r\times (n-r)}\\\,\\
O_{(n-r)\times r}&\alpha^2I_{n-r}\end{bmatrix}\yv
=\begin{bmatrix} \sigma_1z_1\\\vdots\\\sigma_rz_r\\\,\\\ov_{n-r} \end{bmatrix}.
\end{equation*}
Dividing each of the first \(r\)~rows by the corresponding nonzero singular value, \hlist\sigma r, the equivalent system is
\begin{equation*}
\begin{bmatrix} \begin{matrix} \sigma_1+\alpha^2/\sigma_1&\cdots&0\\
\vdots&\ddots&\vdots\\
0&\cdots&\sigma_r+\alpha^2/\sigma_r \end{matrix} & 
O_{r\times (n-r)}\\\,\\
O_{(n-r)\times r}&\alpha^2I_{n-r}\end{bmatrix}\yv
=\begin{bmatrix} z_1\\\vdots\\z_r\\\,\\\ov_{n-r} \end{bmatrix},
\end{equation*}
with solution 
\begin{itemize}
\item \(y_i=z_i/(\sigma_i+\alpha^2/\sigma_i)\) for \(i=1,\ldots,r\)\,, and
\item \(y_i=0\) for \(i=r+1,\ldots,n\) (since \(\alpha^2>0\)).
\end{itemize}
This establishes that solving the Tikhonov system is equivalent to performing the \svd\ \cref{pro:appsol} for the least square solution to \(A\xv=\bv\) but with two changes in \cref{as:a2}:
\begin{itemize}
\item for $i=1,\ldots,r$ divide by \(\sigma'_i:=\sigma_i+\alpha^2/\sigma_i\) instead of the true singular value~\(\sigma_i\) (the upcoming marginal plot shows \(\sigma'\) versus~\(\sigma\)), and
\item for $i=r+1,\ldots,n$ set \(y_i=0\) to obtain the smallest possible solution (\cref{thm:smallsoln}).
\end{itemize}
Thus \idx{Tikhonov regularization} of \(A\xv=\bv\) is equivalent to finding the smallest, {least square}, solution of the system \(A'\xv=\bv\)\,.
\end{proof}



There is another reason to be careful when using Tikhonov regularization. 
Yes, it gives a nice, neat, \idx{unique solution}.
However, it does not hint that there may be an infinite number of equally good nearby solutions (as found through \cref{pro:appmat}).
Among those equally good nearby solutions may be ones that you prefer in your application.

\Needspace{5\baselineskip}
\paragraph{Choose a good regularization parameter}
\begin{itemize}
\item One strategy to choose the \idx{regularization parameter}~\(\alpha\) is that the effective change in the matrix, from~\(A\) to~\(A'\), should be about the magnitude of errors expected in~\(A\).%
\footnote{This strategic choice is sometimes called the \idx{discrepancy principle} \cite[\S7]{Kress2015}.}
Since changes in the matrix are largely measured by the \idx{singular value}s we need to consider the relation between \(\sigma'=\sigma+\alpha^2/\sigma\) and~\(\sigma\).
\marginpar{\begin{tikzpicture}
\begin{axis}[footnotesize,font=\footnotesize
  ,axis lines=middle,ymin=0,xmin=0,axis equal image
  ,xlabel={$\sigma$},ylabel={$\sigma'=\sigma+\alpha^2/\sigma$}
  ,xticklabel={$\ifcase\tick\or\or2\or3\or4\fi\alpha$}
  ,yticklabel={$\ifcase\tick\or\or2\or3\or4\fi\alpha$}
  ]
\addplot[blue,domain=0.21:4.7,smooth] {x+1/x};
\addplot[red,dashed,domain=0:4.8] {x};
\end{axis}
\end{tikzpicture}}
From the marginal graph the small \idx{singular value}s are changed by a lot, but these are the ones for which we want~\(\sigma'\) large in order to give a `\idx{least square}' approximation.
Significantly, the marginal graph also shows that singular values larger than~\(\alpha\) change by less than~\(\alpha\).
Thus the parameter~\(\alpha\) should not be much larger than the expected error in the elements of the matrix~\(A\).

\item Another consideration is the effect of regularization upon errors in the right-hand side vector.
The condition number of~\(A\) may be very bad.
However, as the marginal graph shows the smallest~\(\sigma'\geq2\alpha\).
Thus, in the regularized system the \idx{condition number} of the effective matrix~\(A'\) is approximately~\(\sigma_1/(2\alpha)\).
We need to choose the regularization parameter~\(\alpha\) large enough so that \(\frac{\sigma_1}{2\alpha}\times\)(\idx{relative error} in~\bv) is an acceptable \idx{relative error} in the solution~\xv\ (\cref{thm:erramp}).
It is only when the \idx{regularization parameter}~\(\alpha\) is big enough that the regularization is effective in finding a \idx{least square} approximation. 

\end{itemize}


\index{Tikhonov regularization|)}






\sectionExercises



\begin{exercise} \label{ex:errmats} 
For each of the following matrices, say~\(A\), and right-hand side vectors, say~\(\bv_1\),
solve \(A\xv=\bv_1\)\,.
But suppose the matrix entries come from experiments and are only known to within errors~\(\pm0.05\).
Thus within \idx{experimental error} the given matrices~\(A'\) and~\(A''\) may be the `true' matrix~\(A\).
Solve \(A'\xv'=\bv_1\) and \(A''\xv''=\bv_1\) and comment on the results.
Finally, use an \svd\ to find a \idx{general solution} consistent with the error in the matrix.
%\begin{verbatim} A brutal search for such problems
%format bank
%n=2
%for k=1:999
%  A=0+round(randn(n)*10)/10; 
%  if(min(svd(A))<0.02)&(rcond(A)>0.005)
%  A=A
%  b=0+round(A*randn(n,1)*10)/10, x=(A\b)'
%  Ad=A+round(rand(n)*9-4.5)/100, xd=(Ad\b)'
%  Add=A+round(rand(n)*9-4.5)/100, xdd=(Add\b)'
%  [U,S,V]=svd(A); S=diag(S), k=sum(S>0.05)
%  z=U(:,1:k)'*b; y=z./S(1:k); xsmall=(V(:,1:k)*y)'  
%  Vker=V(:,k+1:end)'
%  break,end
%end
%\end{verbatim}

\begin{enumerate} \raggedright
\item \(\eAii=\begin{bmatrix} -1.3&-0.4\\0.7&0.2 \end{bmatrix}\),
\(\bv_{\arabic{enumii}}=\begin{bmatrix} 2.4\\-1.3 \end{bmatrix}\), 
\(\eAii'=\begin{bmatrix} -1.27&-0.43\\0.71&0.19 \end{bmatrix}\),
\(\eAii''=\begin{bmatrix} -1.27&-0.38\\0.66&0.22 \end{bmatrix}\).
\answer{\(\xv=(-2.00,0.50)\),
\(\xv'=(-1.61,-0.83)\),
\(\xv''=(-1.19,-2.34)\),
\(\xv=(-1.69,-0.51)+t(0.29,-0.96)\) \twodp.}

\item \(\eAii=\begin{bmatrix} -1.8&-1.1
\\-0.2&-0.1 \end{bmatrix}\),
\(\bv_{\arabic{enumii}}=\begin{bmatrix} -0.7
\\-0.1 \end{bmatrix}\), 
\(\eAii'=\begin{bmatrix} -1.81&-1.13
\\-0.24&-0.12 \end{bmatrix}\),
\(\eAii''=\begin{bmatrix} -1.81&-1.13
\\-0.18&-0.1 \end{bmatrix}\).
\answer{\(\xv=(1.0,-1.0)\),
\(\xv'=(0.54,-0.24)\),
\(\xv''=(1.92,-2.46)\),
\(\xv=(0.28,0.17)+t(-0.52,0.85)\) \twodp.}

\begin{reduce}
\item \(\eAii=\begin{bmatrix} 0.8&-0.1
\\-1.0&0.1 \end{bmatrix}\),
\(\bv_{\arabic{enumii}}=\begin{bmatrix} 0.2
\\-0.3 \end{bmatrix}\), 
\(\eAii'=\begin{bmatrix} 0.81&-0.07
\\-1.01&0.06 \end{bmatrix}\),
\(\eAii''=\begin{bmatrix} 0.79&-0.08
\\-1.03&0.09 \end{bmatrix}\).
\answer{\(\xv=(0.5,2.0)\),
\(\xv'=(0.41,1.86)\),
\(\xv''=(0.53,2.74)\),
\(\xv=(0.28,-0.03)+t(-0.11,-0.99)\) \twodp.}

\item \(\eAii=\begin{bmatrix} 0.0&0.5&-0.5
\\0.6&0.5&0.9
\\0.6&1.3&0.0 \end{bmatrix}\),
\(\bv_{\arabic{enumii}}=\begin{bmatrix} -1.4
\\0.4
\\-1.9 \end{bmatrix}\), 
\(\eAii'=\begin{bmatrix} -0.02&0.49&-0.49
\\0.58&0.54&0.9
\\0.61&1.34&-0.02 \end{bmatrix}\),
\(\eAii''=\begin{bmatrix} -0.04&0.52&-0.48
\\0.64&0.52&0.87
\\0.57&1.33&0.04 \end{bmatrix}\).
\answer{\(\xv=(1.6,-2.2,0.6)\),
\(\xv'=(1.31,-2.0,0.8)\),
\(\xv''=(-0.28,-1.35,1.47)\),
\(\xv=(-0.09,-1.43,1.32)+t(0.85,-0.39,-0.36)\) \twodp.}
\end{reduce}

\item \(\eAii=\begin{bmatrix} 0.6&-0.8&-0.2
\\-0.9&1.0&1.2
\\-0.9&0.9&1.4 \end{bmatrix}\),
\(\bv_{\arabic{enumii}}=\begin{bmatrix} 1.1
\\-3.7
\\-4.1 \end{bmatrix}\), 
\(\eAii'=\begin{bmatrix} 0.57&-0.78&-0.23
\\-0.91&0.99&1.22
\\-0.93&0.9&1.39 \end{bmatrix}\),
\(\eAii''=\begin{bmatrix} 0.56&-0.77&-0.21
\\-0.87&1.01&1.22
\\-0.87&0.9&1.39 \end{bmatrix}\).
\answer{\(\xv=(-0.33,-1.0,-2.5)\),
\(\xv'=(0.84,-0.11,-2.31)\),
\(\xv''=(1.54,0.29,-2.17)\),
\(\xv=(0.65,-0.32,-2.32)+t(-0.81,-0.57,-0.15)\) \twodp.}

\begin{reduce}
\item \(\eAii=\begin{bmatrix} 0.1&-1.0&0.0
\\2.1&-0.2&-0.5
\\0.0&-1.6&0.0 \end{bmatrix}\),
\(\bv_{\arabic{enumii}}=\begin{bmatrix} -0.2
\\1.6
\\-0.5 \end{bmatrix}\), 
\(\eAii'=\begin{bmatrix} 0.1&-0.98&-0.04
\\2.11&-0.17&-0.47
\\-0.04&-1.62&-0.01 \end{bmatrix}\),
\(\eAii''=\begin{bmatrix} 0.14&-0.96&0.01
\\2.13&-0.23&-0.47
\\0.0&-1.57&-0.02 \end{bmatrix}\).
\answer{\(\xv=(1.12,0.31,1.4)\),
\(\xv'=(0.59,0.3,-0.84)\),
\(\xv''=(0.77,0.32,-0.08)\),
\(\xv=(0.75,0.3,-0.18)+t(-0.23,-0.01,-0.97)\) \twodp.}

\item \(\eAii=\begin{bmatrix} 1.0&-0.3&0.3&-0.4
\\1.8&0.5&0.1&0.2
\\0.2&-0.3&1.3&-0.6
\\0.0&0.5&1.2&0.0 \end{bmatrix}\),
\(\bv_{\arabic{enumii}}=\begin{bmatrix} 2.0
\\1.6
\\1.4
\\-0.2 \end{bmatrix}\), 
\(\eAii'=\begin{bmatrix} 0.98&-0.3&0.31&-0.44
\\1.8&0.54&0.06&0.21
\\0.24&-0.33&1.27&-0.58
\\0.01&0.52&1.23&-0.01 \end{bmatrix}\),
\(\eAii''=\begin{bmatrix} 1.03&-0.32&0.33&-0.36
\\1.82&0.49&0.08&0.16
\\0.2&-0.31&1.33&-0.64
\\0.0&0.49&1.22&0.0 \end{bmatrix}\).
\answer{\(\xv=(1.21,-0.53,0.06,-1.54)\),
\(\xv'=(1.25,-0.79,0.15,-1.11)\),
\(\xv''=(1.28,-1.56,0.46,-0.07)\),
\(\xv=(1.26,-1.0,0.25,-0.9)+t(0.06,-0.57,0.23,0.79)\) \twodp.}
\end{reduce}

\item \(\eAii=\begin{bmatrix} -0.9&-0.5&-0.3&-0.4
\\-0.1&0.1&-0.2&0.8
\\-1.0&0.4&-1.1&0.6
\\1.0&2.2&-1.0&-0.1 \end{bmatrix}\),
\(\bv_{\arabic{enumii}}=\begin{bmatrix} 0.4
\\0.3
\\0.2
\\-2.0 \end{bmatrix}\), 
\(\eAii'=\begin{bmatrix} -0.88&-0.52&-0.33&-0.41
\\-0.11&0.13&-0.17&0.78
\\-0.96&0.44&-1.12&0.61
\\0.98&2.19&-0.99&-0.13 \end{bmatrix}\),
\(\eAii''=\begin{bmatrix} -0.86&-0.49&-0.29&-0.37
\\-0.06&0.14&-0.18&0.83
\\-0.96&0.38&-1.11&0.58
\\1.01&2.21&-1.04&-0.13 \end{bmatrix}\).
\answer{\(\xv=(-0.76,-0.23,0.69,0.48)\),
\(\xv'=(-1.36,0.33,1.35,0.43)\),
\(\xv''=(-0.09,-0.92,-0.17,0.47)\),
\(\xv=(-0.38,-0.63,0.2,0.47)+t(0.52,-0.54,-0.67,-0.02)\) \twodp.}

\end{enumerate}
\end{exercise}



\begin{exercise} \label{ex:hilb7} 
Recall \cref{eg:hilb5} explores the effective rank of the \(5\times5\) \idx{Hilbert matrix} depending upon a supposed level of error.
Similarly, explore the effective rank of the \(7\times7\) \idx{Hilbert matrix} (\verb|hilb(7)| in \script) depending upon supposed levels of error in the matrix.
What levels of error in the components would give what effective rank of the matrix?
\answer{rank one, \(1>e>0.3\);
two,~\(0.3>e>0.02\);
three,~\(0.02>e>0.001\);
four,~\(0.001>e>3\cdot10^{-5}\);
five,~\(3\cdot10^{-5}>e>5\times 10^{-7}\);
six,~\(5\times 10^{-7}>e>3\cdot10^{-9}\);
seven,~\(3\times 10^{-9}>e\).}
\end{exercise}




% Could do GPS exercises analogous to \cref{ex:gps3t}
% or University ranking exercise





\begin{exercise} \label{ex:orb4Periods2} 
Recall \cref{ex:orb4Periods} considered the inner four \idx{planets} in the \idx{solar system}.
The exercise fitted a quadratic polynomial to the \idx{orbital period} \(T=c_1+c_2R+c_3R^2\) as a function of distance~\(R\) using the data of \cref{tbl:orb4Periods}.
In view of the bad \idx{condition number}, \(\verb|rcond|=6\cdot10^{-6}\), revisit the task with the more powerful techniques of this section.
Use the data for Mercury, Venus and Earth to fit the quadratic and predict the period for Mars.
\index{data scaling}Discuss how the bad condition number is due to the failure in \cref{ex:orb4Periods} of scaling the data in the matrix.
%\begin{verbatim}
%t=[87.97;224.70;365.26]
%r=[57.91;108.21;149.60]
%rs=r/100
%A=[ones(3,1) rs rs.^2]
%rcond(A)
%c=A\t
%tmars=c(1)+c(2)*227.94/100+c(3)*(227.94/100)^2
%relerr=tmars/686.97-1
%\end{verbatim}
\end{exercise}



\begin{exercise}  
\label{ex:ex:ctscan4x4dx}
Recall \cref{ex:ctscan4x4d} used a \(4\times4\) grid of pixels in the \idx{computed tomography} of a \textsc{ct}-scan\index{CT scan}.
Redo this exercise recognizing that the entries in matrix~\(A\) have errors up to roughly~\(0.5\).
Discuss any change in the prediction.
\answer{The matrix has effective rank of eleven.  
Pixel ten is still the most absorbing.  
The corner pixels are the most affected.}
\end{exercise}




\begin{exercise}  
Reconsider each of the matrix-vector \idx{system}s you explored in \cref{ex:errmats}.
Also solve each system using \idx{Tikhonov regularization}; for example, in the first system solve \(A\xv=\bv_1\)\,, \(A'\xv'=\bv_1\) and \(A''\xv''=\bv_1\).
Discuss why \(\xv\), \(\xv'\) and~\(\xv''\) are all reasonably close to the \idx{smallest solution} of those obtained via an \svd.
\end{exercise}




\begin{exercise}  
Recall that \cref{eg:hilb5} explores the effective rank of the \(5\times5\) \idx{Hilbert matrix}, say~\(A\), depending upon a supposed level of error.
Here do the alternative and solve the system \(A\xv=\vec 1\) via \idx{Tikhonov regularization} using a wide range of various \idx{regularization parameter}s~\(\alpha\).
Comment on the relation between the solutions obtained for various~\(\alpha\) and those obtained in the example for the various presumed error---perhaps plot the components of~\xv\ versus parameter~\(\alpha\) (on a \idx{log-log plot}).
\end{exercise}




\begin{exercise}  
Recall \cref{eg:ctscan3x3d2} used a \(3\times3\) grid of pixels in the \idx{computed tomography} of a \textsc{ct}-scan\index{CT scan}.
Redo this example with \idx{Tikhonov regularization} recognizing that the entries in matrix~\(A\) have errors up to roughly~\(0.5\).
Discuss the relation between the solution of \cref{eg:ctscan3x3d2} that of \idx{Tikhonov regularization}.
\end{exercise}




\begin{exercise}  
Recall \cref{ex:ctscan4x4d} used a \(4\times4\) grid of pixels in the \idx{computed tomography} of a \textsc{ct}-scan\index{CT scan}.
Redo this exercise with \idx{Tikhonov regularization} recognizing that the entries in matrix~\(A\) have errors up to roughly~\(0.5\).
Discuss any change in the prediction, and compare to the answer for \cref{ex:ex:ctscan4x4dx}.
\end{exercise}






\begin{exercise} 
In a few sentences, answer\slash discuss each of the following.
\begin{enumerate}
\item  How can errors in the components of a matrix get magnified to badly affect the solution of a system of linear equations?

\item Recall that in some example linear equations, we reported on the variety of solutions found upon changing the matrix by a typical error in the components. 
What is the relation between the variety of solutions found and the solutions predicted by the \svd\ regularization \cref{pro:appmat}?

\item Why does the effective rank of a matrix typically depend upon the expected errors in the matrix?

\item What are the advantages and disadvantages of Tikhonov regularization compared to using an \svd?

\item How is dividing by \(\sigma_i+\alpha^2/\sigma_i\) in Tikhonov regularization roughly like neglecting equations in the \svd\ regularization?

\end{enumerate}
\end{exercise}


\begin{comment}%{ED498555.pdf}
why, what caused X?
how did X occur?
what-if? what-if-not?
how does X compare with Y?
what is the evidence for X?
why is X important?
\end{comment}






\begin{comment}
Possible extensions of this chapter include Frobenius norm.  
Possibly link to polar decomposition (Higham86) 
see closestOrthogonalMatrix.png

\begin{exercise}  
Prove that for every square matrix~\(A\) with \svd\ \(A=\usv\), a closest \idx{orthogonal matrix} to~\(A\) is~\(U\tr V\) ??
Related to the Kabsch algorithm??
\begin{center}
\TwoD1101
\TwoD{0.89443}{0.44721}{0.44721}{1.34164}%H or R
\TwoD{0.89443}{0.44721}{-0.44721}{0.89443}%U or Q
\end{center}
\end{exercise}
\end{comment}


\index{linear equation|)}
\index{regularization|)}
